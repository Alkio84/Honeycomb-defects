{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alkio84/Honeycomb-defects/blob/main/project_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QI9s0nPvck",
        "outputId": "059afe13-cc6b-4d79-b8ad-2db482281dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSazco5NPg1L"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mrcnn pycocotools imgaug==0.2.6 tensorflow-gpu==1.15.3 keras==2.2.4 h5py==2.10.0 scikit-image==0.16.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx4ZlJkdPmq7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import imgaug\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as maskUtils\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = \"/content/drive/MyDrive/Data\"\n",
        "\n",
        "# Import Mask RCNN\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "from mrcnn import visualize\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn.model import log\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_hc.h5\")\n",
        "\n",
        "COCO_DIR = os.path.join(ROOT_DIR)\n",
        "\n",
        "# Path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory to save logs and model checkpoints, if not provided\n",
        "# through the command line argument --logs\n",
        "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wANhjtF9vMg3"
      },
      "outputs": [],
      "source": [
        "class HCConfig(Config):\n",
        "    \"\"\"Configuration for training on MS COCO.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the COCO dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"hc_dataset\"\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    IMAGES_PER_GPU = 1\n",
        "    # GPU_COUNT = 8\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1 # Pitting + Background\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    IMAGE_MIN_DIM = 600\n",
        "    IMAGE_MAX_DIM = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1iHthcBlFE0"
      },
      "outputs": [],
      "source": [
        "config = HCConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF580ftgPg-G"
      },
      "outputs": [],
      "source": [
        "class HCDataset(utils.Dataset):\n",
        "    def load_hc(self, dataset_dir, subset, class_ids=None, return_coco=False):\n",
        "        \"\"\"Load a subset of the COCO dataset.\n",
        "        dataset_dir: The root directory of the COCO dataset.\n",
        "        subset: What to load (train, val, minival, valminusminival)\n",
        "        class_ids: If provided, only loads images that have the given classes.\n",
        "        return_coco: If True, returns the COCO object.\n",
        "        \"\"\"\n",
        "\n",
        "        coco = COCO(os.path.join(dataset_dir, \"labels\", subset + \".json\"))\n",
        "\n",
        "        # Load all classes or a subset?\n",
        "        if not class_ids:\n",
        "            # All classes\n",
        "            class_ids = sorted(coco.getCatIds())\n",
        "\n",
        "        # All images or a subset?\n",
        "        if class_ids:\n",
        "            image_ids = []\n",
        "            for id in class_ids:\n",
        "                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
        "            # Remove duplicates\n",
        "            image_ids = list(set(image_ids))\n",
        "        else:\n",
        "            # All images\n",
        "            image_ids = list(coco.imgs.keys())\n",
        "\n",
        "        # Add classes\n",
        "        for i in class_ids:\n",
        "            self.add_class(\"hc_dataset\", i, coco.loadCats(i)[0][\"name\"])\n",
        "\n",
        "        # Add images\n",
        "        for i in image_ids:\n",
        "            self.add_image(\n",
        "                \"hc_dataset\", image_id=i,\n",
        "                path=coco.imgs[i]['file_name'],\n",
        "                width=coco.imgs[i][\"width\"],\n",
        "                height=coco.imgs[i][\"height\"],\n",
        "                annotations=coco.loadAnns(coco.getAnnIds(\n",
        "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
        "        if return_coco:\n",
        "            return coco\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Load instance masks for the given image.\n",
        "        Different datasets use different ways to store masks. This\n",
        "        function converts the different mask format to one format\n",
        "        in the form of a bitmap [height, width, instances].\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a COCO image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"hc_dataset\":\n",
        "            return super(HCDataset, self).load_mask(image_id)\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = self.map_source_class_id(\n",
        "                \"hc_dataset.{}\".format(annotation['category_id']))\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation, image_info[\"height\"],\n",
        "                                   image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "                # Is it a crowd? If so, use a negative class ID.\n",
        "                if annotation['iscrowd']:\n",
        "                    # Use negative class ID for crowds\n",
        "                    class_id *= -1\n",
        "                    # For crowd masks, annToMask() sometimes returns a mask\n",
        "                    # smaller than the given dimensions. If so, resize it.\n",
        "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
        "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2).astype(bool)\n",
        "            class_ids = np.array(class_ids, dtype=np.int32)\n",
        "            return mask, class_ids\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(HCDataset, self).load_mask(image_id)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"hc_dataset\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(HCDataset, self).image_reference(image_id)\n",
        "\n",
        "    def annToRLE(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        segm = ann['segmentation']\n",
        "        if isinstance(segm, list):\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, height, width)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif isinstance(segm['counts'], list):\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, height, width)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann, height, width)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47jRCs3qUHZ5",
        "outputId": "5ac50866-ef20-4626-910f-2dd864274647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# Training dataset\n",
        "dataset_train = HCDataset()\n",
        "dataset_train.load_hc(ROOT_DIR, \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = HCDataset()\n",
        "dataset_val.load_hc(ROOT_DIR, \"val\")\n",
        "dataset_val.prepare()\n",
        "\n",
        "# Image Augmentation\n",
        "# Right/Left flip 50% of the time\n",
        "augmentation = imgaug.augmenters.Fliplr(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Efxrr5eKx2"
      },
      "source": [
        "# Model + Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pXZLNkCeUZ8"
      },
      "outputs": [],
      "source": [
        "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7GpCf6-eWsz"
      },
      "outputs": [],
      "source": [
        "# Select weights file to load\n",
        "init_with = \"last\"\n",
        "\n",
        "if init_with == \"coco\":\n",
        "    model_path = COCO_MODEL_PATH\n",
        "elif init_with == \"last\":\n",
        "    # Find last trained weights\n",
        "    model_path = model.find_last()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spMJZpoUeWvE",
        "outputId": "5c5b0ed7-9857-48fe-9d3d-443f97f1cc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights  /content/drive/MyDrive/Data/logs/hc_dataset20220518T1403/mask_rcnn_hc_dataset_0020.h5\n",
            "Re-starting from epoch 20\n"
          ]
        }
      ],
      "source": [
        "# Load weights\n",
        "print(\"Loading weights \", model_path)\n",
        "model.load_weights(model_path, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMsZhVoReWzH"
      },
      "outputs": [],
      "source": [
        "# Training - Stage 1\n",
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head layers. \n",
        "# You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "print(\"Training network heads\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=20,\n",
        "            layers='heads',\n",
        "            augmentation=augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlM0ZOM4eanh",
        "outputId": "ab83a52d-2f36-40a1-afb6-7978ec87a512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine tune Resnet stage 4 and up\n",
            "\n",
            "Starting at epoch 20. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/drive/MyDrive/Data/logs/hc_dataset20220518T1403/mask_rcnn_hc_dataset_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "res4a_branch2a         (Conv2D)\n",
            "bn4a_branch2a          (BatchNorm)\n",
            "res4a_branch2b         (Conv2D)\n",
            "bn4a_branch2b          (BatchNorm)\n",
            "res4a_branch2c         (Conv2D)\n",
            "res4a_branch1          (Conv2D)\n",
            "bn4a_branch2c          (BatchNorm)\n",
            "bn4a_branch1           (BatchNorm)\n",
            "res4b_branch2a         (Conv2D)\n",
            "bn4b_branch2a          (BatchNorm)\n",
            "res4b_branch2b         (Conv2D)\n",
            "bn4b_branch2b          (BatchNorm)\n",
            "res4b_branch2c         (Conv2D)\n",
            "bn4b_branch2c          (BatchNorm)\n",
            "res4c_branch2a         (Conv2D)\n",
            "bn4c_branch2a          (BatchNorm)\n",
            "res4c_branch2b         (Conv2D)\n",
            "bn4c_branch2b          (BatchNorm)\n",
            "res4c_branch2c         (Conv2D)\n",
            "bn4c_branch2c          (BatchNorm)\n",
            "res4d_branch2a         (Conv2D)\n",
            "bn4d_branch2a          (BatchNorm)\n",
            "res4d_branch2b         (Conv2D)\n",
            "bn4d_branch2b          (BatchNorm)\n",
            "res4d_branch2c         (Conv2D)\n",
            "bn4d_branch2c          (BatchNorm)\n",
            "res4e_branch2a         (Conv2D)\n",
            "bn4e_branch2a          (BatchNorm)\n",
            "res4e_branch2b         (Conv2D)\n",
            "bn4e_branch2b          (BatchNorm)\n",
            "res4e_branch2c         (Conv2D)\n",
            "bn4e_branch2c          (BatchNorm)\n",
            "res4f_branch2a         (Conv2D)\n",
            "bn4f_branch2a          (BatchNorm)\n",
            "res4f_branch2b         (Conv2D)\n",
            "bn4f_branch2b          (BatchNorm)\n",
            "res4f_branch2c         (Conv2D)\n",
            "bn4f_branch2c          (BatchNorm)\n",
            "res4g_branch2a         (Conv2D)\n",
            "bn4g_branch2a          (BatchNorm)\n",
            "res4g_branch2b         (Conv2D)\n",
            "bn4g_branch2b          (BatchNorm)\n",
            "res4g_branch2c         (Conv2D)\n",
            "bn4g_branch2c          (BatchNorm)\n",
            "res4h_branch2a         (Conv2D)\n",
            "bn4h_branch2a          (BatchNorm)\n",
            "res4h_branch2b         (Conv2D)\n",
            "bn4h_branch2b          (BatchNorm)\n",
            "res4h_branch2c         (Conv2D)\n",
            "bn4h_branch2c          (BatchNorm)\n",
            "res4i_branch2a         (Conv2D)\n",
            "bn4i_branch2a          (BatchNorm)\n",
            "res4i_branch2b         (Conv2D)\n",
            "bn4i_branch2b          (BatchNorm)\n",
            "res4i_branch2c         (Conv2D)\n",
            "bn4i_branch2c          (BatchNorm)\n",
            "res4j_branch2a         (Conv2D)\n",
            "bn4j_branch2a          (BatchNorm)\n",
            "res4j_branch2b         (Conv2D)\n",
            "bn4j_branch2b          (BatchNorm)\n",
            "res4j_branch2c         (Conv2D)\n",
            "bn4j_branch2c          (BatchNorm)\n",
            "res4k_branch2a         (Conv2D)\n",
            "bn4k_branch2a          (BatchNorm)\n",
            "res4k_branch2b         (Conv2D)\n",
            "bn4k_branch2b          (BatchNorm)\n",
            "res4k_branch2c         (Conv2D)\n",
            "bn4k_branch2c          (BatchNorm)\n",
            "res4l_branch2a         (Conv2D)\n",
            "bn4l_branch2a          (BatchNorm)\n",
            "res4l_branch2b         (Conv2D)\n",
            "bn4l_branch2b          (BatchNorm)\n",
            "res4l_branch2c         (Conv2D)\n",
            "bn4l_branch2c          (BatchNorm)\n",
            "res4m_branch2a         (Conv2D)\n",
            "bn4m_branch2a          (BatchNorm)\n",
            "res4m_branch2b         (Conv2D)\n",
            "bn4m_branch2b          (BatchNorm)\n",
            "res4m_branch2c         (Conv2D)\n",
            "bn4m_branch2c          (BatchNorm)\n",
            "res4n_branch2a         (Conv2D)\n",
            "bn4n_branch2a          (BatchNorm)\n",
            "res4n_branch2b         (Conv2D)\n",
            "bn4n_branch2b          (BatchNorm)\n",
            "res4n_branch2c         (Conv2D)\n",
            "bn4n_branch2c          (BatchNorm)\n",
            "res4o_branch2a         (Conv2D)\n",
            "bn4o_branch2a          (BatchNorm)\n",
            "res4o_branch2b         (Conv2D)\n",
            "bn4o_branch2b          (BatchNorm)\n",
            "res4o_branch2c         (Conv2D)\n",
            "bn4o_branch2c          (BatchNorm)\n",
            "res4p_branch2a         (Conv2D)\n",
            "bn4p_branch2a          (BatchNorm)\n",
            "res4p_branch2b         (Conv2D)\n",
            "bn4p_branch2b          (BatchNorm)\n",
            "res4p_branch2c         (Conv2D)\n",
            "bn4p_branch2c          (BatchNorm)\n",
            "res4q_branch2a         (Conv2D)\n",
            "bn4q_branch2a          (BatchNorm)\n",
            "res4q_branch2b         (Conv2D)\n",
            "bn4q_branch2b          (BatchNorm)\n",
            "res4q_branch2c         (Conv2D)\n",
            "bn4q_branch2c          (BatchNorm)\n",
            "res4r_branch2a         (Conv2D)\n",
            "bn4r_branch2a          (BatchNorm)\n",
            "res4r_branch2b         (Conv2D)\n",
            "bn4r_branch2b          (BatchNorm)\n",
            "res4r_branch2c         (Conv2D)\n",
            "bn4r_branch2c          (BatchNorm)\n",
            "res4s_branch2a         (Conv2D)\n",
            "bn4s_branch2a          (BatchNorm)\n",
            "res4s_branch2b         (Conv2D)\n",
            "bn4s_branch2b          (BatchNorm)\n",
            "res4s_branch2c         (Conv2D)\n",
            "bn4s_branch2c          (BatchNorm)\n",
            "res4t_branch2a         (Conv2D)\n",
            "bn4t_branch2a          (BatchNorm)\n",
            "res4t_branch2b         (Conv2D)\n",
            "bn4t_branch2b          (BatchNorm)\n",
            "res4t_branch2c         (Conv2D)\n",
            "bn4t_branch2c          (BatchNorm)\n",
            "res4u_branch2a         (Conv2D)\n",
            "bn4u_branch2a          (BatchNorm)\n",
            "res4u_branch2b         (Conv2D)\n",
            "bn4u_branch2b          (BatchNorm)\n",
            "res4u_branch2c         (Conv2D)\n",
            "bn4u_branch2c          (BatchNorm)\n",
            "res4v_branch2a         (Conv2D)\n",
            "bn4v_branch2a          (BatchNorm)\n",
            "res4v_branch2b         (Conv2D)\n",
            "bn4v_branch2b          (BatchNorm)\n",
            "res4v_branch2c         (Conv2D)\n",
            "bn4v_branch2c          (BatchNorm)\n",
            "res4w_branch2a         (Conv2D)\n",
            "bn4w_branch2a          (BatchNorm)\n",
            "res4w_branch2b         (Conv2D)\n",
            "bn4w_branch2b          (BatchNorm)\n",
            "res4w_branch2c         (Conv2D)\n",
            "bn4w_branch2c          (BatchNorm)\n",
            "res5a_branch2a         (Conv2D)\n",
            "bn5a_branch2a          (BatchNorm)\n",
            "res5a_branch2b         (Conv2D)\n",
            "bn5a_branch2b          (BatchNorm)\n",
            "res5a_branch2c         (Conv2D)\n",
            "res5a_branch1          (Conv2D)\n",
            "bn5a_branch2c          (BatchNorm)\n",
            "bn5a_branch1           (BatchNorm)\n",
            "res5b_branch2a         (Conv2D)\n",
            "bn5b_branch2a          (BatchNorm)\n",
            "res5b_branch2b         (Conv2D)\n",
            "bn5b_branch2b          (BatchNorm)\n",
            "res5b_branch2c         (Conv2D)\n",
            "bn5b_branch2c          (BatchNorm)\n",
            "res5c_branch2a         (Conv2D)\n",
            "bn5c_branch2a          (BatchNorm)\n",
            "res5c_branch2b         (Conv2D)\n",
            "bn5c_branch2b          (BatchNorm)\n",
            "res5c_branch2c         (Conv2D)\n",
            "bn5c_branch2c          (BatchNorm)\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/40\n",
            "100/100 [==============================] - 367s 4s/step - loss: 1.0985 - rpn_class_loss: 0.0303 - rpn_bbox_loss: 0.0943 - mrcnn_class_loss: 0.2253 - mrcnn_bbox_loss: 0.2990 - mrcnn_mask_loss: 0.4497 - val_loss: 0.9565 - val_rpn_class_loss: 0.0298 - val_rpn_bbox_loss: 0.1209 - val_mrcnn_class_loss: 0.1988 - val_mrcnn_bbox_loss: 0.3047 - val_mrcnn_mask_loss: 0.3022\n",
            "Epoch 22/40\n",
            "100/100 [==============================] - 193s 2s/step - loss: 0.7307 - rpn_class_loss: 0.0195 - rpn_bbox_loss: 0.0765 - mrcnn_class_loss: 0.1641 - mrcnn_bbox_loss: 0.2234 - mrcnn_mask_loss: 0.2472 - val_loss: 0.7865 - val_rpn_class_loss: 0.0276 - val_rpn_bbox_loss: 0.0873 - val_mrcnn_class_loss: 0.1480 - val_mrcnn_bbox_loss: 0.2810 - val_mrcnn_mask_loss: 0.2425\n",
            "Epoch 23/40\n",
            "100/100 [==============================] - 196s 2s/step - loss: 0.5216 - rpn_class_loss: 0.0156 - rpn_bbox_loss: 0.0573 - mrcnn_class_loss: 0.1049 - mrcnn_bbox_loss: 0.1489 - mrcnn_mask_loss: 0.1949 - val_loss: 0.5892 - val_rpn_class_loss: 0.0224 - val_rpn_bbox_loss: 0.0688 - val_mrcnn_class_loss: 0.1426 - val_mrcnn_bbox_loss: 0.1550 - val_mrcnn_mask_loss: 0.2004\n",
            "Epoch 24/40\n",
            "100/100 [==============================] - 197s 2s/step - loss: 0.4088 - rpn_class_loss: 0.0111 - rpn_bbox_loss: 0.0460 - mrcnn_class_loss: 0.0924 - mrcnn_bbox_loss: 0.1031 - mrcnn_mask_loss: 0.1563 - val_loss: 0.4226 - val_rpn_class_loss: 0.0111 - val_rpn_bbox_loss: 0.0575 - val_mrcnn_class_loss: 0.0887 - val_mrcnn_bbox_loss: 0.1033 - val_mrcnn_mask_loss: 0.1621\n",
            "Epoch 25/40\n",
            "100/100 [==============================] - 196s 2s/step - loss: 0.3461 - rpn_class_loss: 0.0072 - rpn_bbox_loss: 0.0358 - mrcnn_class_loss: 0.0761 - mrcnn_bbox_loss: 0.0856 - mrcnn_mask_loss: 0.1414 - val_loss: 0.4461 - val_rpn_class_loss: 0.0131 - val_rpn_bbox_loss: 0.0499 - val_mrcnn_class_loss: 0.1124 - val_mrcnn_bbox_loss: 0.1136 - val_mrcnn_mask_loss: 0.1571\n",
            "Epoch 26/40\n",
            "100/100 [==============================] - 225s 2s/step - loss: 0.3364 - rpn_class_loss: 0.0097 - rpn_bbox_loss: 0.0383 - mrcnn_class_loss: 0.0708 - mrcnn_bbox_loss: 0.0824 - mrcnn_mask_loss: 0.1353 - val_loss: 0.4675 - val_rpn_class_loss: 0.0130 - val_rpn_bbox_loss: 0.0714 - val_mrcnn_class_loss: 0.0940 - val_mrcnn_bbox_loss: 0.1149 - val_mrcnn_mask_loss: 0.1741\n",
            "Epoch 27/40\n",
            "100/100 [==============================] - 234s 2s/step - loss: 0.3433 - rpn_class_loss: 0.0069 - rpn_bbox_loss: 0.0379 - mrcnn_class_loss: 0.0640 - mrcnn_bbox_loss: 0.0980 - mrcnn_mask_loss: 0.1365 - val_loss: 0.3755 - val_rpn_class_loss: 0.0101 - val_rpn_bbox_loss: 0.0569 - val_mrcnn_class_loss: 0.0848 - val_mrcnn_bbox_loss: 0.0793 - val_mrcnn_mask_loss: 0.1444\n",
            "Epoch 28/40\n",
            "100/100 [==============================] - 231s 2s/step - loss: 0.3885 - rpn_class_loss: 0.0075 - rpn_bbox_loss: 0.0364 - mrcnn_class_loss: 0.0649 - mrcnn_bbox_loss: 0.0982 - mrcnn_mask_loss: 0.1815 - val_loss: 0.4565 - val_rpn_class_loss: 0.0118 - val_rpn_bbox_loss: 0.0561 - val_mrcnn_class_loss: 0.1068 - val_mrcnn_bbox_loss: 0.1169 - val_mrcnn_mask_loss: 0.1650\n",
            "Epoch 29/40\n",
            "100/100 [==============================] - 232s 2s/step - loss: 0.3037 - rpn_class_loss: 0.0054 - rpn_bbox_loss: 0.0348 - mrcnn_class_loss: 0.0539 - mrcnn_bbox_loss: 0.0819 - mrcnn_mask_loss: 0.1276 - val_loss: 0.4232 - val_rpn_class_loss: 0.0117 - val_rpn_bbox_loss: 0.0625 - val_mrcnn_class_loss: 0.0811 - val_mrcnn_bbox_loss: 0.1155 - val_mrcnn_mask_loss: 0.1524\n",
            "Epoch 30/40\n",
            "100/100 [==============================] - 221s 2s/step - loss: 0.2866 - rpn_class_loss: 0.0039 - rpn_bbox_loss: 0.0355 - mrcnn_class_loss: 0.0523 - mrcnn_bbox_loss: 0.0707 - mrcnn_mask_loss: 0.1242 - val_loss: 0.3714 - val_rpn_class_loss: 0.0066 - val_rpn_bbox_loss: 0.0525 - val_mrcnn_class_loss: 0.0707 - val_mrcnn_bbox_loss: 0.0889 - val_mrcnn_mask_loss: 0.1526\n",
            "Epoch 31/40\n",
            "100/100 [==============================] - 226s 2s/step - loss: 0.3084 - rpn_class_loss: 0.0053 - rpn_bbox_loss: 0.0289 - mrcnn_class_loss: 0.0548 - mrcnn_bbox_loss: 0.0783 - mrcnn_mask_loss: 0.1411 - val_loss: 0.4080 - val_rpn_class_loss: 0.0120 - val_rpn_bbox_loss: 0.0591 - val_mrcnn_class_loss: 0.0912 - val_mrcnn_bbox_loss: 0.0959 - val_mrcnn_mask_loss: 0.1497\n",
            "Epoch 32/40\n",
            "100/100 [==============================] - 227s 2s/step - loss: 0.2923 - rpn_class_loss: 0.0037 - rpn_bbox_loss: 0.0314 - mrcnn_class_loss: 0.0585 - mrcnn_bbox_loss: 0.0647 - mrcnn_mask_loss: 0.1340 - val_loss: 0.3235 - val_rpn_class_loss: 0.0051 - val_rpn_bbox_loss: 0.0448 - val_mrcnn_class_loss: 0.0573 - val_mrcnn_bbox_loss: 0.0740 - val_mrcnn_mask_loss: 0.1422\n",
            "Epoch 33/40\n",
            "100/100 [==============================] - 228s 2s/step - loss: 0.2880 - rpn_class_loss: 0.0036 - rpn_bbox_loss: 0.0273 - mrcnn_class_loss: 0.0512 - mrcnn_bbox_loss: 0.0654 - mrcnn_mask_loss: 0.1405 - val_loss: 0.3769 - val_rpn_class_loss: 0.0061 - val_rpn_bbox_loss: 0.0683 - val_mrcnn_class_loss: 0.0780 - val_mrcnn_bbox_loss: 0.0787 - val_mrcnn_mask_loss: 0.1457\n",
            "Epoch 34/40\n",
            "100/100 [==============================] - 228s 2s/step - loss: 0.2827 - rpn_class_loss: 0.0033 - rpn_bbox_loss: 0.0407 - mrcnn_class_loss: 0.0504 - mrcnn_bbox_loss: 0.0601 - mrcnn_mask_loss: 0.1282 - val_loss: 0.3910 - val_rpn_class_loss: 0.0096 - val_rpn_bbox_loss: 0.0544 - val_mrcnn_class_loss: 0.0770 - val_mrcnn_bbox_loss: 0.0837 - val_mrcnn_mask_loss: 0.1663\n",
            "Epoch 35/40\n",
            "100/100 [==============================] - 222s 2s/step - loss: 0.2671 - rpn_class_loss: 0.0035 - rpn_bbox_loss: 0.0297 - mrcnn_class_loss: 0.0511 - mrcnn_bbox_loss: 0.0582 - mrcnn_mask_loss: 0.1245 - val_loss: 0.3377 - val_rpn_class_loss: 0.0066 - val_rpn_bbox_loss: 0.0509 - val_mrcnn_class_loss: 0.0491 - val_mrcnn_bbox_loss: 0.0929 - val_mrcnn_mask_loss: 0.1381\n",
            "Epoch 36/40\n",
            "100/100 [==============================] - 228s 2s/step - loss: 0.2275 - rpn_class_loss: 0.0025 - rpn_bbox_loss: 0.0261 - mrcnn_class_loss: 0.0423 - mrcnn_bbox_loss: 0.0513 - mrcnn_mask_loss: 0.1053 - val_loss: 0.3618 - val_rpn_class_loss: 0.0044 - val_rpn_bbox_loss: 0.0454 - val_mrcnn_class_loss: 0.0532 - val_mrcnn_bbox_loss: 0.0786 - val_mrcnn_mask_loss: 0.1802\n",
            "Epoch 37/40\n",
            "100/100 [==============================] - 225s 2s/step - loss: 0.2816 - rpn_class_loss: 0.0027 - rpn_bbox_loss: 0.0292 - mrcnn_class_loss: 0.0496 - mrcnn_bbox_loss: 0.0726 - mrcnn_mask_loss: 0.1275 - val_loss: 0.3540 - val_rpn_class_loss: 0.0084 - val_rpn_bbox_loss: 0.0594 - val_mrcnn_class_loss: 0.0612 - val_mrcnn_bbox_loss: 0.0885 - val_mrcnn_mask_loss: 0.1365\n",
            "Epoch 38/40\n",
            "100/100 [==============================] - 230s 2s/step - loss: 0.2540 - rpn_class_loss: 0.0026 - rpn_bbox_loss: 0.0270 - mrcnn_class_loss: 0.0428 - mrcnn_bbox_loss: 0.0589 - mrcnn_mask_loss: 0.1227 - val_loss: 0.4802 - val_rpn_class_loss: 0.0159 - val_rpn_bbox_loss: 0.0516 - val_mrcnn_class_loss: 0.1433 - val_mrcnn_bbox_loss: 0.0851 - val_mrcnn_mask_loss: 0.1843\n",
            "Epoch 39/40\n",
            "100/100 [==============================] - 220s 2s/step - loss: 0.2424 - rpn_class_loss: 0.0033 - rpn_bbox_loss: 0.0230 - mrcnn_class_loss: 0.0409 - mrcnn_bbox_loss: 0.0540 - mrcnn_mask_loss: 0.1213 - val_loss: 0.3298 - val_rpn_class_loss: 0.0071 - val_rpn_bbox_loss: 0.0520 - val_mrcnn_class_loss: 0.0694 - val_mrcnn_bbox_loss: 0.0569 - val_mrcnn_mask_loss: 0.1444\n",
            "Epoch 40/40\n",
            "100/100 [==============================] - 225s 2s/step - loss: 0.2025 - rpn_class_loss: 0.0018 - rpn_bbox_loss: 0.0186 - mrcnn_class_loss: 0.0363 - mrcnn_bbox_loss: 0.0368 - mrcnn_mask_loss: 0.1091 - val_loss: 0.3292 - val_rpn_class_loss: 0.0056 - val_rpn_bbox_loss: 0.0497 - val_mrcnn_class_loss: 0.0744 - val_mrcnn_bbox_loss: 0.0623 - val_mrcnn_mask_loss: 0.1372\n"
          ]
        }
      ],
      "source": [
        "# Training - Stage 2\n",
        "# Finetune layers from ResNet stage 4 and up\n",
        "print(\"Fine tune Resnet stage 4 and up\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=40,\n",
        "            layers='4+',\n",
        "            augmentation=augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFUGr_Z3eapy",
        "outputId": "376ae774-9da7-440a-8090-50f21405ae3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine tune all layers\n",
            "\n",
            "Starting at epoch 40. LR=0.0001\n",
            "\n",
            "Checkpoint Path: /content/drive/MyDrive/Data/logs/hc_dataset20220518T1403/mask_rcnn_hc_dataset_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "conv1                  (Conv2D)\n",
            "bn_conv1               (BatchNorm)\n",
            "res2a_branch2a         (Conv2D)\n",
            "bn2a_branch2a          (BatchNorm)\n",
            "res2a_branch2b         (Conv2D)\n",
            "bn2a_branch2b          (BatchNorm)\n",
            "res2a_branch2c         (Conv2D)\n",
            "res2a_branch1          (Conv2D)\n",
            "bn2a_branch2c          (BatchNorm)\n",
            "bn2a_branch1           (BatchNorm)\n",
            "res2b_branch2a         (Conv2D)\n",
            "bn2b_branch2a          (BatchNorm)\n",
            "res2b_branch2b         (Conv2D)\n",
            "bn2b_branch2b          (BatchNorm)\n",
            "res2b_branch2c         (Conv2D)\n",
            "bn2b_branch2c          (BatchNorm)\n",
            "res2c_branch2a         (Conv2D)\n",
            "bn2c_branch2a          (BatchNorm)\n",
            "res2c_branch2b         (Conv2D)\n",
            "bn2c_branch2b          (BatchNorm)\n",
            "res2c_branch2c         (Conv2D)\n",
            "bn2c_branch2c          (BatchNorm)\n",
            "res3a_branch2a         (Conv2D)\n",
            "bn3a_branch2a          (BatchNorm)\n",
            "res3a_branch2b         (Conv2D)\n",
            "bn3a_branch2b          (BatchNorm)\n",
            "res3a_branch2c         (Conv2D)\n",
            "res3a_branch1          (Conv2D)\n",
            "bn3a_branch2c          (BatchNorm)\n",
            "bn3a_branch1           (BatchNorm)\n",
            "res3b_branch2a         (Conv2D)\n",
            "bn3b_branch2a          (BatchNorm)\n",
            "res3b_branch2b         (Conv2D)\n",
            "bn3b_branch2b          (BatchNorm)\n",
            "res3b_branch2c         (Conv2D)\n",
            "bn3b_branch2c          (BatchNorm)\n",
            "res3c_branch2a         (Conv2D)\n",
            "bn3c_branch2a          (BatchNorm)\n",
            "res3c_branch2b         (Conv2D)\n",
            "bn3c_branch2b          (BatchNorm)\n",
            "res3c_branch2c         (Conv2D)\n",
            "bn3c_branch2c          (BatchNorm)\n",
            "res3d_branch2a         (Conv2D)\n",
            "bn3d_branch2a          (BatchNorm)\n",
            "res3d_branch2b         (Conv2D)\n",
            "bn3d_branch2b          (BatchNorm)\n",
            "res3d_branch2c         (Conv2D)\n",
            "bn3d_branch2c          (BatchNorm)\n",
            "res4a_branch2a         (Conv2D)\n",
            "bn4a_branch2a          (BatchNorm)\n",
            "res4a_branch2b         (Conv2D)\n",
            "bn4a_branch2b          (BatchNorm)\n",
            "res4a_branch2c         (Conv2D)\n",
            "res4a_branch1          (Conv2D)\n",
            "bn4a_branch2c          (BatchNorm)\n",
            "bn4a_branch1           (BatchNorm)\n",
            "res4b_branch2a         (Conv2D)\n",
            "bn4b_branch2a          (BatchNorm)\n",
            "res4b_branch2b         (Conv2D)\n",
            "bn4b_branch2b          (BatchNorm)\n",
            "res4b_branch2c         (Conv2D)\n",
            "bn4b_branch2c          (BatchNorm)\n",
            "res4c_branch2a         (Conv2D)\n",
            "bn4c_branch2a          (BatchNorm)\n",
            "res4c_branch2b         (Conv2D)\n",
            "bn4c_branch2b          (BatchNorm)\n",
            "res4c_branch2c         (Conv2D)\n",
            "bn4c_branch2c          (BatchNorm)\n",
            "res4d_branch2a         (Conv2D)\n",
            "bn4d_branch2a          (BatchNorm)\n",
            "res4d_branch2b         (Conv2D)\n",
            "bn4d_branch2b          (BatchNorm)\n",
            "res4d_branch2c         (Conv2D)\n",
            "bn4d_branch2c          (BatchNorm)\n",
            "res4e_branch2a         (Conv2D)\n",
            "bn4e_branch2a          (BatchNorm)\n",
            "res4e_branch2b         (Conv2D)\n",
            "bn4e_branch2b          (BatchNorm)\n",
            "res4e_branch2c         (Conv2D)\n",
            "bn4e_branch2c          (BatchNorm)\n",
            "res4f_branch2a         (Conv2D)\n",
            "bn4f_branch2a          (BatchNorm)\n",
            "res4f_branch2b         (Conv2D)\n",
            "bn4f_branch2b          (BatchNorm)\n",
            "res4f_branch2c         (Conv2D)\n",
            "bn4f_branch2c          (BatchNorm)\n",
            "res4g_branch2a         (Conv2D)\n",
            "bn4g_branch2a          (BatchNorm)\n",
            "res4g_branch2b         (Conv2D)\n",
            "bn4g_branch2b          (BatchNorm)\n",
            "res4g_branch2c         (Conv2D)\n",
            "bn4g_branch2c          (BatchNorm)\n",
            "res4h_branch2a         (Conv2D)\n",
            "bn4h_branch2a          (BatchNorm)\n",
            "res4h_branch2b         (Conv2D)\n",
            "bn4h_branch2b          (BatchNorm)\n",
            "res4h_branch2c         (Conv2D)\n",
            "bn4h_branch2c          (BatchNorm)\n",
            "res4i_branch2a         (Conv2D)\n",
            "bn4i_branch2a          (BatchNorm)\n",
            "res4i_branch2b         (Conv2D)\n",
            "bn4i_branch2b          (BatchNorm)\n",
            "res4i_branch2c         (Conv2D)\n",
            "bn4i_branch2c          (BatchNorm)\n",
            "res4j_branch2a         (Conv2D)\n",
            "bn4j_branch2a          (BatchNorm)\n",
            "res4j_branch2b         (Conv2D)\n",
            "bn4j_branch2b          (BatchNorm)\n",
            "res4j_branch2c         (Conv2D)\n",
            "bn4j_branch2c          (BatchNorm)\n",
            "res4k_branch2a         (Conv2D)\n",
            "bn4k_branch2a          (BatchNorm)\n",
            "res4k_branch2b         (Conv2D)\n",
            "bn4k_branch2b          (BatchNorm)\n",
            "res4k_branch2c         (Conv2D)\n",
            "bn4k_branch2c          (BatchNorm)\n",
            "res4l_branch2a         (Conv2D)\n",
            "bn4l_branch2a          (BatchNorm)\n",
            "res4l_branch2b         (Conv2D)\n",
            "bn4l_branch2b          (BatchNorm)\n",
            "res4l_branch2c         (Conv2D)\n",
            "bn4l_branch2c          (BatchNorm)\n",
            "res4m_branch2a         (Conv2D)\n",
            "bn4m_branch2a          (BatchNorm)\n",
            "res4m_branch2b         (Conv2D)\n",
            "bn4m_branch2b          (BatchNorm)\n",
            "res4m_branch2c         (Conv2D)\n",
            "bn4m_branch2c          (BatchNorm)\n",
            "res4n_branch2a         (Conv2D)\n",
            "bn4n_branch2a          (BatchNorm)\n",
            "res4n_branch2b         (Conv2D)\n",
            "bn4n_branch2b          (BatchNorm)\n",
            "res4n_branch2c         (Conv2D)\n",
            "bn4n_branch2c          (BatchNorm)\n",
            "res4o_branch2a         (Conv2D)\n",
            "bn4o_branch2a          (BatchNorm)\n",
            "res4o_branch2b         (Conv2D)\n",
            "bn4o_branch2b          (BatchNorm)\n",
            "res4o_branch2c         (Conv2D)\n",
            "bn4o_branch2c          (BatchNorm)\n",
            "res4p_branch2a         (Conv2D)\n",
            "bn4p_branch2a          (BatchNorm)\n",
            "res4p_branch2b         (Conv2D)\n",
            "bn4p_branch2b          (BatchNorm)\n",
            "res4p_branch2c         (Conv2D)\n",
            "bn4p_branch2c          (BatchNorm)\n",
            "res4q_branch2a         (Conv2D)\n",
            "bn4q_branch2a          (BatchNorm)\n",
            "res4q_branch2b         (Conv2D)\n",
            "bn4q_branch2b          (BatchNorm)\n",
            "res4q_branch2c         (Conv2D)\n",
            "bn4q_branch2c          (BatchNorm)\n",
            "res4r_branch2a         (Conv2D)\n",
            "bn4r_branch2a          (BatchNorm)\n",
            "res4r_branch2b         (Conv2D)\n",
            "bn4r_branch2b          (BatchNorm)\n",
            "res4r_branch2c         (Conv2D)\n",
            "bn4r_branch2c          (BatchNorm)\n",
            "res4s_branch2a         (Conv2D)\n",
            "bn4s_branch2a          (BatchNorm)\n",
            "res4s_branch2b         (Conv2D)\n",
            "bn4s_branch2b          (BatchNorm)\n",
            "res4s_branch2c         (Conv2D)\n",
            "bn4s_branch2c          (BatchNorm)\n",
            "res4t_branch2a         (Conv2D)\n",
            "bn4t_branch2a          (BatchNorm)\n",
            "res4t_branch2b         (Conv2D)\n",
            "bn4t_branch2b          (BatchNorm)\n",
            "res4t_branch2c         (Conv2D)\n",
            "bn4t_branch2c          (BatchNorm)\n",
            "res4u_branch2a         (Conv2D)\n",
            "bn4u_branch2a          (BatchNorm)\n",
            "res4u_branch2b         (Conv2D)\n",
            "bn4u_branch2b          (BatchNorm)\n",
            "res4u_branch2c         (Conv2D)\n",
            "bn4u_branch2c          (BatchNorm)\n",
            "res4v_branch2a         (Conv2D)\n",
            "bn4v_branch2a          (BatchNorm)\n",
            "res4v_branch2b         (Conv2D)\n",
            "bn4v_branch2b          (BatchNorm)\n",
            "res4v_branch2c         (Conv2D)\n",
            "bn4v_branch2c          (BatchNorm)\n",
            "res4w_branch2a         (Conv2D)\n",
            "bn4w_branch2a          (BatchNorm)\n",
            "res4w_branch2b         (Conv2D)\n",
            "bn4w_branch2b          (BatchNorm)\n",
            "res4w_branch2c         (Conv2D)\n",
            "bn4w_branch2c          (BatchNorm)\n",
            "res5a_branch2a         (Conv2D)\n",
            "bn5a_branch2a          (BatchNorm)\n",
            "res5a_branch2b         (Conv2D)\n",
            "bn5a_branch2b          (BatchNorm)\n",
            "res5a_branch2c         (Conv2D)\n",
            "res5a_branch1          (Conv2D)\n",
            "bn5a_branch2c          (BatchNorm)\n",
            "bn5a_branch1           (BatchNorm)\n",
            "res5b_branch2a         (Conv2D)\n",
            "bn5b_branch2a          (BatchNorm)\n",
            "res5b_branch2b         (Conv2D)\n",
            "bn5b_branch2b          (BatchNorm)\n",
            "res5b_branch2c         (Conv2D)\n",
            "bn5b_branch2c          (BatchNorm)\n",
            "res5c_branch2a         (Conv2D)\n",
            "bn5c_branch2a          (BatchNorm)\n",
            "res5c_branch2b         (Conv2D)\n",
            "bn5c_branch2b          (BatchNorm)\n",
            "res5c_branch2c         (Conv2D)\n",
            "bn5c_branch2c          (BatchNorm)\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/60\n",
            "100/100 [==============================] - 410s 4s/step - loss: 0.1968 - rpn_class_loss: 0.0025 - rpn_bbox_loss: 0.0176 - mrcnn_class_loss: 0.0348 - mrcnn_bbox_loss: 0.0316 - mrcnn_mask_loss: 0.1101 - val_loss: 0.2884 - val_rpn_class_loss: 0.0078 - val_rpn_bbox_loss: 0.0387 - val_mrcnn_class_loss: 0.0604 - val_mrcnn_bbox_loss: 0.0535 - val_mrcnn_mask_loss: 0.1280\n",
            "Epoch 42/60\n",
            "100/100 [==============================] - 244s 2s/step - loss: 0.1537 - rpn_class_loss: 0.0015 - rpn_bbox_loss: 0.0127 - mrcnn_class_loss: 0.0273 - mrcnn_bbox_loss: 0.0219 - mrcnn_mask_loss: 0.0902 - val_loss: 0.2660 - val_rpn_class_loss: 0.0068 - val_rpn_bbox_loss: 0.0372 - val_mrcnn_class_loss: 0.0434 - val_mrcnn_bbox_loss: 0.0505 - val_mrcnn_mask_loss: 0.1282\n",
            "Epoch 43/60\n",
            "100/100 [==============================] - 258s 3s/step - loss: 0.1594 - rpn_class_loss: 0.0011 - rpn_bbox_loss: 0.0116 - mrcnn_class_loss: 0.0285 - mrcnn_bbox_loss: 0.0228 - mrcnn_mask_loss: 0.0954 - val_loss: 0.2914 - val_rpn_class_loss: 0.0050 - val_rpn_bbox_loss: 0.0412 - val_mrcnn_class_loss: 0.0591 - val_mrcnn_bbox_loss: 0.0532 - val_mrcnn_mask_loss: 0.1328\n",
            "Epoch 44/60\n",
            "100/100 [==============================] - 254s 3s/step - loss: 0.1657 - rpn_class_loss: 0.0016 - rpn_bbox_loss: 0.0112 - mrcnn_class_loss: 0.0296 - mrcnn_bbox_loss: 0.0229 - mrcnn_mask_loss: 0.1004 - val_loss: 0.2881 - val_rpn_class_loss: 0.0058 - val_rpn_bbox_loss: 0.0394 - val_mrcnn_class_loss: 0.0677 - val_mrcnn_bbox_loss: 0.0503 - val_mrcnn_mask_loss: 0.1248\n",
            "Epoch 45/60\n",
            "100/100 [==============================] - 246s 2s/step - loss: 0.1289 - rpn_class_loss: 0.0012 - rpn_bbox_loss: 0.0090 - mrcnn_class_loss: 0.0229 - mrcnn_bbox_loss: 0.0141 - mrcnn_mask_loss: 0.0818 - val_loss: 0.2818 - val_rpn_class_loss: 0.0067 - val_rpn_bbox_loss: 0.0358 - val_mrcnn_class_loss: 0.0602 - val_mrcnn_bbox_loss: 0.0481 - val_mrcnn_mask_loss: 0.1310\n",
            "Epoch 46/60\n",
            "100/100 [==============================] - 252s 3s/step - loss: 0.1496 - rpn_class_loss: 0.0017 - rpn_bbox_loss: 0.0087 - mrcnn_class_loss: 0.0280 - mrcnn_bbox_loss: 0.0196 - mrcnn_mask_loss: 0.0915 - val_loss: 0.3092 - val_rpn_class_loss: 0.0048 - val_rpn_bbox_loss: 0.0401 - val_mrcnn_class_loss: 0.0737 - val_mrcnn_bbox_loss: 0.0544 - val_mrcnn_mask_loss: 0.1361\n",
            "Epoch 47/60\n",
            "100/100 [==============================] - 254s 3s/step - loss: 0.1389 - rpn_class_loss: 0.0015 - rpn_bbox_loss: 0.0092 - mrcnn_class_loss: 0.0248 - mrcnn_bbox_loss: 0.0158 - mrcnn_mask_loss: 0.0876 - val_loss: 0.2786 - val_rpn_class_loss: 0.0056 - val_rpn_bbox_loss: 0.0380 - val_mrcnn_class_loss: 0.0612 - val_mrcnn_bbox_loss: 0.0480 - val_mrcnn_mask_loss: 0.1260\n",
            "Epoch 48/60\n",
            "100/100 [==============================] - 255s 3s/step - loss: 0.1453 - rpn_class_loss: 0.0019 - rpn_bbox_loss: 0.0082 - mrcnn_class_loss: 0.0260 - mrcnn_bbox_loss: 0.0185 - mrcnn_mask_loss: 0.0908 - val_loss: 0.3039 - val_rpn_class_loss: 0.0064 - val_rpn_bbox_loss: 0.0374 - val_mrcnn_class_loss: 0.0749 - val_mrcnn_bbox_loss: 0.0500 - val_mrcnn_mask_loss: 0.1352\n",
            "Epoch 49/60\n",
            "100/100 [==============================] - 257s 3s/step - loss: 0.1226 - rpn_class_loss: 0.0013 - rpn_bbox_loss: 0.0079 - mrcnn_class_loss: 0.0217 - mrcnn_bbox_loss: 0.0129 - mrcnn_mask_loss: 0.0788 - val_loss: 0.2936 - val_rpn_class_loss: 0.0056 - val_rpn_bbox_loss: 0.0344 - val_mrcnn_class_loss: 0.0693 - val_mrcnn_bbox_loss: 0.0500 - val_mrcnn_mask_loss: 0.1344\n",
            "Epoch 50/60\n",
            "100/100 [==============================] - 242s 2s/step - loss: 0.1239 - rpn_class_loss: 0.0010 - rpn_bbox_loss: 0.0078 - mrcnn_class_loss: 0.0216 - mrcnn_bbox_loss: 0.0128 - mrcnn_mask_loss: 0.0807 - val_loss: 0.2909 - val_rpn_class_loss: 0.0054 - val_rpn_bbox_loss: 0.0368 - val_mrcnn_class_loss: 0.0665 - val_mrcnn_bbox_loss: 0.0493 - val_mrcnn_mask_loss: 0.1330\n",
            "Epoch 51/60\n",
            "100/100 [==============================] - 246s 2s/step - loss: 0.1207 - rpn_class_loss: 0.0011 - rpn_bbox_loss: 0.0070 - mrcnn_class_loss: 0.0203 - mrcnn_bbox_loss: 0.0128 - mrcnn_mask_loss: 0.0794 - val_loss: 0.3198 - val_rpn_class_loss: 0.0066 - val_rpn_bbox_loss: 0.0369 - val_mrcnn_class_loss: 0.0854 - val_mrcnn_bbox_loss: 0.0534 - val_mrcnn_mask_loss: 0.1375\n",
            "Epoch 52/60\n",
            "100/100 [==============================] - 246s 2s/step - loss: 0.1220 - rpn_class_loss: 9.6221e-04 - rpn_bbox_loss: 0.0069 - mrcnn_class_loss: 0.0208 - mrcnn_bbox_loss: 0.0138 - mrcnn_mask_loss: 0.0795 - val_loss: 0.3000 - val_rpn_class_loss: 0.0051 - val_rpn_bbox_loss: 0.0358 - val_mrcnn_class_loss: 0.0684 - val_mrcnn_bbox_loss: 0.0529 - val_mrcnn_mask_loss: 0.1379\n",
            "Epoch 53/60\n",
            "100/100 [==============================] - 247s 2s/step - loss: 0.1237 - rpn_class_loss: 8.8188e-04 - rpn_bbox_loss: 0.0067 - mrcnn_class_loss: 0.0193 - mrcnn_bbox_loss: 0.0147 - mrcnn_mask_loss: 0.0822 - val_loss: 0.3010 - val_rpn_class_loss: 0.0045 - val_rpn_bbox_loss: 0.0352 - val_mrcnn_class_loss: 0.0705 - val_mrcnn_bbox_loss: 0.0513 - val_mrcnn_mask_loss: 0.1394\n",
            "Epoch 54/60\n",
            "100/100 [==============================] - 297s 3s/step - loss: 0.1277 - rpn_class_loss: 0.0010 - rpn_bbox_loss: 0.0076 - mrcnn_class_loss: 0.0216 - mrcnn_bbox_loss: 0.0147 - mrcnn_mask_loss: 0.0828 - val_loss: 0.3056 - val_rpn_class_loss: 0.0056 - val_rpn_bbox_loss: 0.0353 - val_mrcnn_class_loss: 0.0703 - val_mrcnn_bbox_loss: 0.0526 - val_mrcnn_mask_loss: 0.1417\n",
            "Epoch 55/60\n",
            "100/100 [==============================] - 256s 3s/step - loss: 0.1208 - rpn_class_loss: 0.0014 - rpn_bbox_loss: 0.0070 - mrcnn_class_loss: 0.0220 - mrcnn_bbox_loss: 0.0123 - mrcnn_mask_loss: 0.0780 - val_loss: 0.3078 - val_rpn_class_loss: 0.0056 - val_rpn_bbox_loss: 0.0365 - val_mrcnn_class_loss: 0.0798 - val_mrcnn_bbox_loss: 0.0511 - val_mrcnn_mask_loss: 0.1349\n",
            "Epoch 56/60\n",
            "100/100 [==============================] - 261s 3s/step - loss: 0.1170 - rpn_class_loss: 0.0010 - rpn_bbox_loss: 0.0066 - mrcnn_class_loss: 0.0209 - mrcnn_bbox_loss: 0.0117 - mrcnn_mask_loss: 0.0767 - val_loss: 0.3042 - val_rpn_class_loss: 0.0046 - val_rpn_bbox_loss: 0.0365 - val_mrcnn_class_loss: 0.0663 - val_mrcnn_bbox_loss: 0.0552 - val_mrcnn_mask_loss: 0.1416\n",
            "Epoch 57/60\n",
            "100/100 [==============================] - 254s 3s/step - loss: 0.1139 - rpn_class_loss: 7.2578e-04 - rpn_bbox_loss: 0.0059 - mrcnn_class_loss: 0.0197 - mrcnn_bbox_loss: 0.0121 - mrcnn_mask_loss: 0.0754 - val_loss: 0.3190 - val_rpn_class_loss: 0.0052 - val_rpn_bbox_loss: 0.0353 - val_mrcnn_class_loss: 0.0819 - val_mrcnn_bbox_loss: 0.0544 - val_mrcnn_mask_loss: 0.1421\n",
            "Epoch 58/60\n",
            "100/100 [==============================] - 258s 3s/step - loss: 0.1204 - rpn_class_loss: 8.3220e-04 - rpn_bbox_loss: 0.0063 - mrcnn_class_loss: 0.0201 - mrcnn_bbox_loss: 0.0141 - mrcnn_mask_loss: 0.0790 - val_loss: 0.3103 - val_rpn_class_loss: 0.0051 - val_rpn_bbox_loss: 0.0366 - val_mrcnn_class_loss: 0.0739 - val_mrcnn_bbox_loss: 0.0556 - val_mrcnn_mask_loss: 0.1392\n",
            "Epoch 59/60\n",
            "100/100 [==============================] - 251s 3s/step - loss: 0.1144 - rpn_class_loss: 0.0011 - rpn_bbox_loss: 0.0056 - mrcnn_class_loss: 0.0196 - mrcnn_bbox_loss: 0.0127 - mrcnn_mask_loss: 0.0754 - val_loss: 0.3095 - val_rpn_class_loss: 0.0057 - val_rpn_bbox_loss: 0.0373 - val_mrcnn_class_loss: 0.0733 - val_mrcnn_bbox_loss: 0.0520 - val_mrcnn_mask_loss: 0.1411\n",
            "Epoch 60/60\n",
            "100/100 [==============================] - 256s 3s/step - loss: 0.1141 - rpn_class_loss: 8.0256e-04 - rpn_bbox_loss: 0.0061 - mrcnn_class_loss: 0.0190 - mrcnn_bbox_loss: 0.0116 - mrcnn_mask_loss: 0.0767 - val_loss: 0.3004 - val_rpn_class_loss: 0.0055 - val_rpn_bbox_loss: 0.0375 - val_mrcnn_class_loss: 0.0712 - val_mrcnn_bbox_loss: 0.0521 - val_mrcnn_mask_loss: 0.1341\n"
          ]
        }
      ],
      "source": [
        "# Training - Stage 3\n",
        "# Fine tune all layers\n",
        "print(\"Fine tune all layers\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=60,\n",
        "            layers='all',\n",
        "            augmentation=augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7gk3Ct3easD"
      },
      "outputs": [],
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZaKQKTSekRR"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdU3y9IwedBe"
      },
      "outputs": [],
      "source": [
        "class InferenceConfig(HCConfig):\n",
        "    # Set batch size to 1 since we'll be running inference on\n",
        "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    DETECTION_MIN_CONFIDENCE = 0\n",
        "inference_config  = InferenceConfig()\n",
        "inference_config .display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxgwVwyJedEB"
      },
      "outputs": [],
      "source": [
        "model = modellib.MaskRCNN(mode=\"inference\", config=inference_config,\n",
        "                          model_dir=DEFAULT_LOGS_DIR)\n",
        "model_path = os.path.join(ROOT_DIR, \"mask_rcnn_hc.h5\")\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FYnj7aredGU"
      },
      "outputs": [],
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "print(\"image_id \", image_id, dataset_val.image_reference(image_id))\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIJtAWQnedIT"
      },
      "outputs": [],
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgCkOGwUeisT"
      },
      "outputs": [],
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project_train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN92eqU65xw2j/uN5UipMSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}